{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study\n",
    "\n",
    "Study effects of different parts from feature engineering.\n",
    "\n",
    "Model: Least Square.\n",
    "\n",
    "- Full feature engineering with normalization: $x := (x-\\min(x))/(\\max(x)-\\min(x))$ (FFE+N)\n",
    "- Full feature engineering with standardization: $x := (x-\\mu(x))/\\sigma(x)$ (FFE+S)\n",
    "- No clipping outliers with normalization (FFE+N-O)\n",
    "- Remove the first column `DER_mass_MMC` with normalization (FFE+N-O-FC)\n",
    "- Remove the first column and no polynomial expansion with normalization (FFE+N-O-FC-PE)\n",
    "- Remove all columns with `-999` and no polynomial expansion with normalization (FFE+N-O-ALLNAN-PE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from utils.helpers import *\n",
    "from utils.prediction import *\n",
    "from utils.preprocess import *\n",
    "from utils.cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./data/train.csv\"\n",
    "TEST_PATH = \"./data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "degree = 9\n",
    "learning_rate = 0.1\n",
    "max_iter = 2000\n",
    "k_fold = 5\n",
    "seed = 20221031\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_plot(y_tr, tx_tr, y_dev, tx_dev, lambda_=0):\n",
    "    \"\"\"Ridge regression using normal equations.\n",
    "    Args:\n",
    "        y: numpy array of shape (N, 1), N is the number of samples.\n",
    "        tx: numpy array of shape (N, D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "\n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D, 1), D is the number of features.\n",
    "        loss: scalar\n",
    "    \"\"\"\n",
    "    N, D = tx_tr.shape\n",
    "    I = np.eye(D)\n",
    "    w = np.linalg.solve(tx_tr.T @ tx_tr + 2 * N * lambda_ * I, tx_tr.T @ y_tr).reshape(\n",
    "        -1, 1\n",
    "    )\n",
    "    train_loss = compute_mse(y_tr, tx_tr, w)\n",
    "    dev_loss = compute_mse(y_dev, tx_dev, w)\n",
    "\n",
    "    return w, train_loss, dev_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFE+S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.61216 0.4005994005994006 0.26299680818503784 0.3175315419944043\n",
      "Validation\n",
      "0.61388 0.40045968882602545 0.26562683241468277 0.3193964605513643\n"
     ]
    }
   ],
   "source": [
    "y_raw_tr, tx_raw_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "_, tx_raw_te, ids_te = load_csv_data(TEST_PATH)\n",
    "\n",
    "y_tr = process_y(y_raw_tr)\n",
    "tx_tr = tx_raw_tr\n",
    "tx_te = tx_raw_te\n",
    "\n",
    "tx_tr[:, [22, 29]] = tx_tr[:, [29, 22]]\n",
    "tx_te[:, [22, 29]] = tx_te[:, [29, 22]]\n",
    "tx_tr[tx_tr[:, 0] == -999, 0] = np.nan\n",
    "tx_te[tx_te[:, 0] == -999, 0] = np.nan\n",
    "\n",
    "median = np.nanmedian(np.hstack((tx_tr[:, 0], tx_te[:, 0])))\n",
    "tx_tr[np.isnan(tx_tr[:, 0]), 0] = median\n",
    "tx_te[np.isnan(tx_te[:, 0]), 0] = median\n",
    "\n",
    "# cross validation\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "tx_tr, tx_dev, y_tr, y_dev = cross_validation_dataset(\n",
    "    y_tr, tx_tr, k_indices, k=k_fold - 1\n",
    ")\n",
    "\n",
    "# split datasets to different jet nums\n",
    "# and remove columns with missing values for each jet num\n",
    "tx_train_list, y_tr_list = split_jet_num(tx_tr, y_tr)\n",
    "tx_dev_list, y_dev_list = split_jet_num(tx_dev, y_dev)\n",
    "\n",
    "# remove outliers\n",
    "means = []\n",
    "stds = []\n",
    "for i in range(3):\n",
    "    mean = np.mean(tx_train_list[i], axis=0)\n",
    "    std = np.std(tx_train_list[i], axis=0)\n",
    "    tx_train_list[i] = np.clip(tx_train_list[i], mean - 2 * std, mean + 2 * std)\n",
    "    tx_dev_list[i] = np.clip(tx_dev_list[i], mean - 2 * std, mean + 2 * std)\n",
    "    means.append(mean)\n",
    "    stds.append(std)\n",
    "\n",
    "# add polynomial features\n",
    "for i in range(3):\n",
    "    tx_train_list[i] = build_poly(tx_train_list[i], degree)\n",
    "    tx_dev_list[i] = build_poly(tx_dev_list[i], degree)\n",
    "\n",
    "means = [0, 0, 0]\n",
    "stds = [0, 0, 0]\n",
    "for i in range(3):\n",
    "    tx_train_list[i], tx_dev_list[i], means[i], stds[i] = standardization(\n",
    "        tx_train_list[i], tx_dev_list[i]\n",
    "    )\n",
    "\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    best_w, train_loss, dev_loss = ridge_regression_plot(\n",
    "        y_tr,\n",
    "        tx_tr_fe,\n",
    "        y_dev,\n",
    "        tx_dev_fe,\n",
    "    )\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)\n",
    "\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFE+N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.83079 0.7790923538760375 0.7073295148149767 0.741478618245153\n",
      "Validation\n",
      "0.83004 0.7738445781590065 0.7088659552011258 0.7399314481576692\n"
     ]
    }
   ],
   "source": [
    "y_raw_tr, tx_raw_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "_, tx_raw_te, ids_te = load_csv_data(TEST_PATH)\n",
    "\n",
    "y_tr = process_y(y_raw_tr)\n",
    "tx_tr = tx_raw_tr\n",
    "tx_te = tx_raw_te\n",
    "\n",
    "tx_tr[:, [22, 29]] = tx_tr[:, [29, 22]]\n",
    "tx_te[:, [22, 29]] = tx_te[:, [29, 22]]\n",
    "tx_tr[tx_tr[:, 0] == -999, 0] = np.nan\n",
    "tx_te[tx_te[:, 0] == -999, 0] = np.nan\n",
    "\n",
    "median = np.nanmedian(np.hstack((tx_tr[:, 0], tx_te[:, 0])))\n",
    "tx_tr[np.isnan(tx_tr[:, 0]), 0] = median\n",
    "tx_te[np.isnan(tx_te[:, 0]), 0] = median\n",
    "\n",
    "# cross validation\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "tx_tr, tx_dev, y_tr, y_dev = cross_validation_dataset(\n",
    "    y_tr, tx_tr, k_indices, k=k_fold - 1\n",
    ")\n",
    "\n",
    "# split datasets to different jet nums\n",
    "# and remove columns with missing values for each jet num\n",
    "tx_train_list, y_tr_list = split_jet_num(tx_tr, y_tr)\n",
    "tx_dev_list, y_dev_list = split_jet_num(tx_dev, y_dev)\n",
    "\n",
    "# remove outliers\n",
    "means = []\n",
    "stds = []\n",
    "for i in range(3):\n",
    "    mean = np.mean(tx_train_list[i], axis=0)\n",
    "    std = np.std(tx_train_list[i], axis=0)\n",
    "    tx_train_list[i] = np.clip(tx_train_list[i], mean - 2 * std, mean + 2 * std)\n",
    "    tx_dev_list[i] = np.clip(tx_dev_list[i], mean - 2 * std, mean + 2 * std)\n",
    "    means.append(mean)\n",
    "    stds.append(std)\n",
    "\n",
    "# add polynomial features\n",
    "for i in range(3):\n",
    "    tx_train_list[i] = build_poly(tx_train_list[i], degree)\n",
    "    tx_dev_list[i] = build_poly(tx_dev_list[i], degree)\n",
    "\n",
    "means = [0, 0, 0]\n",
    "stds = [0, 0, 0]\n",
    "for i in range(3):\n",
    "    tx_train_list[i], tx_dev_list[i], means[i], stds[i] = normalization(\n",
    "        tx_train_list[i], tx_dev_list[i]\n",
    "    )\n",
    "\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    best_w, train_loss, dev_loss = ridge_regression_plot(\n",
    "        y_tr, tx_tr_fe, y_dev, tx_dev_fe, lambda_=1e-8\n",
    "    )\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)\n",
    "\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFE+N-O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw_tr, tx_raw_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "_, tx_raw_te, ids_te = load_csv_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = process_y(y_raw_tr)\n",
    "tx_tr = tx_raw_tr\n",
    "tx_te = tx_raw_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_tr[:, [22, 29]] = tx_tr[:, [29, 22]]\n",
    "tx_te[:, [22, 29]] = tx_te[:, [29, 22]]\n",
    "tx_tr[tx_tr[:, 0] == -999, 0] = 60\n",
    "tx_te[tx_te[:, 0] == -999, 0] = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "tx_tr, tx_dev, y_tr, y_dev = cross_validation_dataset(\n",
    "    y_tr, tx_tr, k_indices, k=k_fold - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split datasets to different jet nums\n",
    "# and remove columns with missing values for each jet num\n",
    "tx_train_list, y_tr_list = split_jet_num(tx_tr, y_tr)\n",
    "tx_dev_list, y_dev_list = split_jet_num(tx_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add polynomial features\n",
    "for i in range(3):\n",
    "    tx_train_list[i] = build_poly(tx_train_list[i], degree)\n",
    "    tx_dev_list[i] = build_poly(tx_dev_list[i], degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = [0, 0, 0]\n",
    "mins = [0, 0, 0]\n",
    "for i in range(3):\n",
    "    tx_train_list[i], tx_dev_list[i], maxs[i], mins[i] = normalization(\n",
    "        tx_train_list[i], tx_dev_list[i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    best_w, train_loss, dev_loss = ridge_regression_plot(\n",
    "        y_tr,\n",
    "        tx_tr_fe,\n",
    "        y_dev,\n",
    "        tx_dev_fe,\n",
    "    )\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.82581 0.7660244797655992 0.7087286665792197 0.7362635698820537\n",
      "Validation\n",
      "0.82624 0.7634130982367758 0.7108596223759822 0.7361996720714157\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFE+N-O-FC\n",
    "\n",
    "Note that here we change polynomial term (`degree`) from 9 to 7 for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.804635 0.7437615526802218 0.6568143063267894 0.6975891025889092\n",
      "Validation\n",
      "0.80522 0.7409261576971214 0.6595520112583558 0.697874980611137\n"
     ]
    }
   ],
   "source": [
    "y_raw_tr, tx_raw_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "_, tx_raw_te, ids_te = load_csv_data(TEST_PATH)\n",
    "\n",
    "y_tr = process_y(y_raw_tr)\n",
    "tx_tr = process_tx2(tx_raw_tr)\n",
    "tx_te = process_tx2(tx_raw_te)\n",
    "\n",
    "# cross validation\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "tx_tr, tx_dev, y_tr, y_dev = cross_validation_dataset(\n",
    "    y_tr, tx_tr, k_indices, k=k_fold - 1\n",
    ")\n",
    "\n",
    "# split datasets to different jet nums\n",
    "# and remove columns with missing values for each jet num\n",
    "tx_train_list, y_tr_list = split_jet_num2(tx_tr, y_tr)\n",
    "tx_dev_list, y_dev_list = split_jet_num2(tx_dev, y_dev)\n",
    "\n",
    "# add polynomial features\n",
    "for i in range(3):\n",
    "    tx_train_list[i] = build_poly(tx_train_list[i], 7)\n",
    "    tx_dev_list[i] = build_poly(tx_dev_list[i], 7)\n",
    "\n",
    "maxs = [0, 0, 0]\n",
    "mins = [0, 0, 0]\n",
    "for i in range(3):\n",
    "    tx_train_list[i], tx_dev_list[i], maxs[i], mins[i] = normalization(\n",
    "        tx_train_list[i], tx_dev_list[i]\n",
    "    )\n",
    "\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    best_w, train_loss, dev_loss = ridge_regression_plot(\n",
    "        y_tr,\n",
    "        tx_tr_fe,\n",
    "        y_dev,\n",
    "        tx_dev_fe,\n",
    "    )\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)\n",
    "\n",
    "# add polynomial features\n",
    "for i in range(3):\n",
    "    tx_train_list[i] = build_poly(tx_train_list[i], degree)\n",
    "    tx_dev_list[i] = build_poly(tx_dev_list[i], degree)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFE+N-O-FC-PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.758595 0.6930350903843232 0.5319400113681081 0.6018948357891438\n",
      "Validation\n",
      "0.7601 0.6903167556993455 0.5379969508619679 0.6047124732245839\n"
     ]
    }
   ],
   "source": [
    "y_raw_tr, tx_raw_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "_, tx_raw_te, ids_te = load_csv_data(TEST_PATH)\n",
    "\n",
    "y_tr = process_y(y_raw_tr)\n",
    "tx_tr = process_tx2(tx_raw_tr)\n",
    "tx_te = process_tx2(tx_raw_te)\n",
    "\n",
    "# cross validation\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "tx_tr, tx_dev, y_tr, y_dev = cross_validation_dataset(\n",
    "    y_tr, tx_tr, k_indices, k=k_fold - 1\n",
    ")\n",
    "\n",
    "# split datasets to different jet nums\n",
    "# and remove columns with missing values for each jet num\n",
    "tx_train_list, y_tr_list = split_jet_num2(tx_tr, y_tr)\n",
    "tx_dev_list, y_dev_list = split_jet_num2(tx_dev, y_dev)\n",
    "\n",
    "maxs = [0, 0, 0]\n",
    "mins = [0, 0, 0]\n",
    "for i in range(3):\n",
    "    tx_train_list[i], tx_dev_list[i], maxs[i], mins[i] = normalization(\n",
    "        tx_train_list[i], tx_dev_list[i]\n",
    "    )\n",
    "\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    best_w, train_loss, dev_loss = ridge_regression_plot(\n",
    "        y_tr,\n",
    "        tx_tr_fe,\n",
    "        y_dev,\n",
    "        tx_dev_fe,\n",
    "    )\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFE+N-O-ALLNAN-PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.74205 0.6557941171087072 0.5221750980134959 0.5814062931048471\n",
      "Validation\n",
      "0.74138 0.6502441513009256 0.5231617215902428 0.5798212835093419\n"
     ]
    }
   ],
   "source": [
    "y_raw_tr, tx_raw_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "_, tx_raw_te, ids_te = load_csv_data(TEST_PATH)\n",
    "\n",
    "y_tr = process_y(y_raw_tr)\n",
    "\n",
    "# mask\n",
    "tx_raw_tr[:, [22, 29]] = tx_raw_tr[:, [29, 22]]\n",
    "tx_raw_te[:, [22, 29]] = tx_raw_te[:, [29, 22]]\n",
    "col_mask = np.zeros(tx_raw_tr.shape[1], dtype=bool)\n",
    "col_mask[[1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 29]] = True\n",
    "tx_tr = tx_raw_tr[:, col_mask]\n",
    "tx_te = tx_raw_te[:, col_mask]\n",
    "\n",
    "# cross validation\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "tx_tr, tx_dev, y_tr, y_dev = cross_validation_dataset(\n",
    "    y_tr, tx_tr, k_indices, k=k_fold - 1\n",
    ")\n",
    "\n",
    "# split datasets to different jet nums\n",
    "# and remove columns with missing values and constant values for each jet num\n",
    "tx_train_list = [tx_tr[tx_tr[:, -1] == 0, :-2]]\n",
    "y_tr_list = [y_tr[tx_tr[:, -1] == 0]]\n",
    "tx_train_list.append(tx_tr[tx_tr[:, -1] >= 1, :-1])\n",
    "y_tr_list.append(y_tr[tx_tr[:, -1] >= 1])\n",
    "\n",
    "tx_dev_list = [tx_dev[tx_dev[:, -1] == 0, :-2]]\n",
    "y_dev_list = [y_dev[tx_dev[:, -1] == 0]]\n",
    "tx_dev_list.append(tx_dev[tx_dev[:, -1] >= 1, :-1])\n",
    "y_dev_list.append(y_dev[tx_dev[:, -1] >= 1])\n",
    "\n",
    "maxs = [0, 0]\n",
    "mins = [0, 0]\n",
    "for i in range(2):\n",
    "    tx_train_list[i], tx_dev_list[i], maxs[i], mins[i] = normalization(\n",
    "        tx_train_list[i], tx_dev_list[i]\n",
    "    )\n",
    "\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    best_w, train_loss, dev_loss = ridge_regression_plot(\n",
    "        y_tr,\n",
    "        tx_tr_fe,\n",
    "        y_dev,\n",
    "        tx_dev_fe,\n",
    "    )\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "578317ea115a054ac4bccdfa892faa859649678dbaf6df54b2b98fb2846a81cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
