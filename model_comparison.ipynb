{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Different Models\n",
    "\n",
    "$\\lambda=1$, $k=5$, $\\text{degree}=9$\n",
    "\n",
    "Using all feature engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from utils.helpers import *\n",
    "from utils.prediction import *\n",
    "from utils.preprocess import *\n",
    "from utils.cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './data/train.csv'\n",
    "TEST_PATH = './data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "degree = 9\n",
    "learning_rate = 0.1\n",
    "max_iter = 2000\n",
    "k_fold = 5\n",
    "seed = 20221031\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw_tr, tx_raw_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "_, tx_raw_te, ids_te = load_csv_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 1)\n",
      "(250000, 30)\n",
      "(568238, 30)\n"
     ]
    }
   ],
   "source": [
    "y_tr = process_y(y_raw_tr)\n",
    "tx_tr = tx_raw_tr\n",
    "tx_te = tx_raw_te\n",
    "print(y_tr.shape)\n",
    "print(tx_tr.shape)\n",
    "print(tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_tr[:, [22, 29]] = tx_tr[:, [29, 22]]\n",
    "tx_te[:, [22, 29]] = tx_te[:, [29, 22]]\n",
    "tx_tr[tx_tr[:, 0] == -999, 0] = 60\n",
    "tx_te[tx_te[:, 0] == -999, 0] = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 30)\n",
      "(50000, 30)\n",
      "(200000, 1)\n",
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "tx_tr, tx_dev, y_tr, y_dev = cross_validation_dataset(y_tr, tx_tr, k_indices, k=k_fold-1)\n",
    "print(tx_tr.shape)\n",
    "print(tx_dev.shape)\n",
    "print(y_tr.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split datasets to different jet nums\n",
    "# and remove columns with missing values for each jet num\n",
    "tx_train_list, y_tr_list = split_jet_num(tx_tr, y_tr)\n",
    "tx_dev_list, y_dev_list = split_jet_num(tx_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add polynomial features\n",
    "for i in range(3):\n",
    "    tx_train_list[i] = build_poly(tx_train_list[i], degree)\n",
    "    tx_dev_list[i] = build_poly(tx_dev_list[i], degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79917, 162) (19996, 162)\n",
      "(62257, 198) (15287, 198)\n",
      "(57826, 261) (14717, 261)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tx_train_list[i].shape, tx_dev_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = [0, 0, 0]\n",
    "mins = [0, 0, 0]\n",
    "for i in range(3):\n",
    "    tx_train_list[i], tx_dev_list[i], maxs[i], mins[i] = normalization(\n",
    "        tx_train_list[i],\n",
    "        tx_dev_list[i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_plot(y_tr, tx_tr, y_dev, tx_dev, lambda_):\n",
    "    \"\"\"Ridge regression using normal equations.\n",
    "    Args:\n",
    "        y: numpy array of shape (N, 1), N is the number of samples.\n",
    "        tx: numpy array of shape (N, D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "\n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D, 1), D is the number of features.\n",
    "        loss: scalar\n",
    "    \"\"\"\n",
    "    N, D = tx_tr.shape\n",
    "    I = np.eye(D)\n",
    "    w = np.linalg.solve(tx_tr.T @ tx_tr + 2 * N * lambda_ * I, tx_tr.T @ y_tr).reshape(-1, 1)\n",
    "    train_loss = compute_mse(y_tr, tx_tr, w)\n",
    "    dev_loss = compute_mse(y_dev, tx_dev, w)\n",
    "\n",
    "    return w, train_loss, dev_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best lambda for PRI_JET_NUM = 0 is 0.\n",
      "The best lambda for PRI_JET_NUM = 1 is 0.\n",
      "The best lambda for PRI_JET_NUM = 2 is 0.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    w_list = []\n",
    "    lambda_list = [0]\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    for lambda_ in lambda_list:\n",
    "        w, train_loss, dev_loss = ridge_regression_plot(\n",
    "            y_tr, tx_tr_fe,\n",
    "            y_dev, tx_dev_fe,\n",
    "            lambda_,\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        dev_losses.append(dev_loss)\n",
    "        w_list.append(w)\n",
    "\n",
    "    # cross_validation_visualization(lambda_list, train_losses, dev_losses, i)\n",
    "    index = np.argmin(dev_losses)\n",
    "    best_lambda = lambda_list[index]\n",
    "    best_w = w_list[index]\n",
    "    print(\"The best lambda for PRI_JET_NUM = {} is {}.\".format(i, best_lambda))\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.82581 0.7660244797655992 0.7087286665792197 0.7362635698820537\n",
      "Validation\n",
      "0.82624 0.7634130982367758 0.7108596223759822 0.7361996720714157\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best lambda for PRI_JET_NUM = 0 is 1e-06.\n",
      "The best lambda for PRI_JET_NUM = 1 is 1e-10.\n",
      "The best lambda for PRI_JET_NUM = 2 is 1e-10.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    w_list = []\n",
    "    lambda_list = np.logspace(-10, 1, 12)\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    for lambda_ in lambda_list:\n",
    "        w, train_loss, dev_loss = ridge_regression_plot(\n",
    "            y_tr, tx_tr_fe,\n",
    "            y_dev, tx_dev_fe,\n",
    "            lambda_,\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        dev_losses.append(dev_loss)\n",
    "        w_list.append(w)\n",
    "\n",
    "    # cross_validation_visualization(lambda_list, train_losses, dev_losses, i)\n",
    "    index = np.argmin(dev_losses)\n",
    "    best_lambda = lambda_list[index]\n",
    "    best_w = w_list[index]\n",
    "    print(\"The best lambda for PRI_JET_NUM = {} is {}.\".format(i, best_lambda))\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.807565 0.7492718607268154 0.659889525308615 0.7017459567114328\n",
      "Validation\n",
      "0.80748 0.7452132576257758 0.6618388647824557 0.7010559006211181\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_sgd(y_tr, tx_tr, y_dev, tx_dev, initial_w, max_iters, gamma, batch_size=1):\n",
    "    \"\"\"Linear regression using stochastic gradient descent.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, 1)\n",
    "        tx: numpy array of shape=(N, D)\n",
    "        initial_w: numpy array of shape=(D, 1). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        batch_size: default 1, a scalar denoting the batch size\n",
    "\n",
    "    Returns:\n",
    "        w: the last weight vector of shape (D, 1)\n",
    "        loss: the corresponding mse loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    train_loss = compute_mse(y_tr, tx_tr, w)\n",
    "    ws = [initial_w]\n",
    "    train_losses = [train_loss]\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # implement stochastic gradient descent.\n",
    "        for y_batch, tx_batch in batch_iter(y_tr, tx_tr, batch_size=batch_size, num_batches=1):\n",
    "\n",
    "            # compute gradient\n",
    "            grad = linear_reg_gradient(y_batch, tx_batch, w)\n",
    "\n",
    "            # update w by gradient\n",
    "            w = w - gamma * grad\n",
    "\n",
    "            # compute loss\n",
    "            train_loss = compute_mse(y_tr, tx_tr, w)\n",
    "            dev_loss = compute_mse(y_dev, tx_dev, w)\n",
    "\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            train_losses.append(train_loss)\n",
    "            dev_losses.append(dev_loss)\n",
    "\n",
    "    index = np.argmin(dev_losses)\n",
    "    return ws[index], train_losses[index], dev_losses[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "    initial_w = np.random.rand(tx_tr_fe.shape[1], 1)\n",
    "\n",
    "    best_w, train_loss, dev_loss = mean_squared_error_sgd(\n",
    "        y_tr, tx_tr_fe,\n",
    "        y_dev, tx_dev_fe,\n",
    "        initial_w,\n",
    "        max_iter,\n",
    "        learning_rate,\n",
    "        batch_size=tx_tr_fe.shape[0]\n",
    "    )\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.73056 0.6746779283019764 0.4144549866643347 0.5134793521243748\n",
      "Validation\n",
      "0.72848 0.6633784291619692 0.4140377624017826 0.5098563073146075\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "    initial_w = np.random.rand(tx_tr_fe.shape[1], 1)\n",
    "\n",
    "    best_w, train_loss, dev_loss = mean_squared_error_sgd(\n",
    "        y_tr, tx_tr_fe,\n",
    "        y_dev, tx_dev_fe,\n",
    "        initial_w,\n",
    "        max_iter,\n",
    "        learning_rate,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.636595 0.46797253889273793 0.4331540670135397 0.4498906305583518\n",
      "Validation\n",
      "0.63978 0.4700694314130231 0.4406590829130996 0.4548893798613844\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression_plot(y_tr, tx_tr, y_dev, tx_dev, lambda_, initial_w, max_iters, gamma, batch_size=8):\n",
    "    \"\"\"Regularized logistic regression using gradient descent\n",
    "    or SGD (y ∈ {0, 1}, with regularization term λ|w|2)\n",
    "\n",
    "    Args:\n",
    "        y_tr: numpy array of shape=(N_tr, 1)\n",
    "        tx_tr: numpy array of shape=(N_tr, D)\n",
    "        y_dev: numpy array of shape=(N_dev, 1)\n",
    "        tx_dev: numpy array of shape=(N_dev, D)\n",
    "        lambda_: a scalar denoting the regularization term\n",
    "        initial_w: numpy array of shape=(D, 1). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        batch_size: mini batch size. default 8.\n",
    "        optimizer: 'gd' (batch sgd), 'ada' (adagrad), and 'adam'. default 'gd'.\n",
    "\n",
    "    Returns:\n",
    "        w: the best weight vector of shape (D, 1) for validation\n",
    "        train_loss: the corresponding mse loss\n",
    "        dev_loss: the corresponding mse loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    ws = [initial_w]\n",
    "    train_losses = [compute_ce(y_tr, tx_tr, w)]\n",
    "    dev_losses = [compute_ce(y_dev, tx_dev, w)]\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(\n",
    "            y_tr, tx_tr, batch_size=batch_size, num_batches=1\n",
    "        ):\n",
    "            # compute gradient\n",
    "            grad = logistic_reg_gradient(y_batch, tx_batch, w)\n",
    "\n",
    "            # update w by gradient\n",
    "            w = w - gamma * (grad + 2 * lambda_ * w)\n",
    "\n",
    "            # compute loss\n",
    "            loss = compute_ce(y_tr, tx_tr, w)\n",
    "\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "            # compute dev loss\n",
    "            dev_losses.append(compute_ce(y_dev, tx_dev, w))\n",
    "    \n",
    "    index = np.argmin(dev_losses)\n",
    "    return ws[index], train_losses[index], dev_losses[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best lambda for PRI_JET_NUM = 0 is 0.\n",
      "The best lambda for PRI_JET_NUM = 1 is 0.\n",
      "The best lambda for PRI_JET_NUM = 2 is 0.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    w_list = []\n",
    "    lambda_list = [0]\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    for lambda_ in lambda_list:\n",
    "        initial_w = np.random.rand(tx_tr_fe.shape[1], 1)\n",
    "        w, train_loss, dev_loss = reg_logistic_regression_plot(\n",
    "            y_tr, tx_tr_fe,\n",
    "            y_dev, tx_dev_fe,\n",
    "            lambda_,\n",
    "            initial_w,\n",
    "            max_iter,\n",
    "            learning_rate,\n",
    "            batch_size=tx_tr_fe.shape[0],\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        dev_losses.append(dev_loss)\n",
    "        w_list.append(w)\n",
    "\n",
    "    index = np.argmin(dev_losses)\n",
    "    best_lambda = lambda_list[index]\n",
    "    best_w = w_list[index]\n",
    "    print(\"The best lambda for PRI_JET_NUM = {} is {}.\".format(i, best_lambda))\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_logistic(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_logistic(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.720055 0.663879945996469 0.3726699022051215 0.47736840631388333\n",
      "Validation\n",
      "0.72064 0.6596647350993378 0.3738125952855635 0.47720637772288343\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best lambda for PRI_JET_NUM = 0 is 0.\n",
      "The best lambda for PRI_JET_NUM = 1 is 0.\n",
      "The best lambda for PRI_JET_NUM = 2 is 0.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    w_list = []\n",
    "    lambda_list = [0]\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    for lambda_ in lambda_list:\n",
    "        initial_w = np.random.rand(tx_tr_fe.shape[1], 1)\n",
    "        w, train_loss, dev_loss = reg_logistic_regression_plot(\n",
    "            y_tr, tx_tr_fe,\n",
    "            y_dev, tx_dev_fe,\n",
    "            lambda_,\n",
    "            initial_w,\n",
    "            max_iter,\n",
    "            learning_rate,\n",
    "            batch_size=1,\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        dev_losses.append(dev_loss)\n",
    "        w_list.append(w)\n",
    "\n",
    "    index = np.argmin(dev_losses)\n",
    "    best_lambda = lambda_list[index]\n",
    "    best_w = w_list[index]\n",
    "    print(\"The best lambda for PRI_JET_NUM = {} is {}.\".format(i, best_lambda))\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_logistic(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_logistic(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.70925 0.6225433932210536 0.38734642123212804 0.4775569611155035\n",
      "Validation\n",
      "0.71058 0.6206896551724138 0.38946874633517065 0.4786164655017115\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Logistic Regression GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best lambda for PRI_JET_NUM = 0 is 1e-07.\n",
      "The best lambda for PRI_JET_NUM = 1 is 1e-05.\n",
      "The best lambda for PRI_JET_NUM = 2 is 1e-08.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    w_list = []\n",
    "    lambda_list = np.logspace(-10, 1, 12)\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    for lambda_ in lambda_list:\n",
    "        initial_w = np.random.rand(tx_tr_fe.shape[1], 1)\n",
    "        w, train_loss, dev_loss = reg_logistic_regression_plot(\n",
    "            y_tr, tx_tr_fe,\n",
    "            y_dev, tx_dev_fe,\n",
    "            lambda_,\n",
    "            initial_w,\n",
    "            max_iter,\n",
    "            learning_rate,\n",
    "            batch_size=tx_tr_fe.shape[0],\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        dev_losses.append(dev_loss)\n",
    "        w_list.append(w)\n",
    "\n",
    "    index = np.argmin(dev_losses)\n",
    "    best_lambda = lambda_list[index]\n",
    "    best_w = w_list[index]\n",
    "    print(\"The best lambda for PRI_JET_NUM = {} is {}.\".format(i, best_lambda))\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_logistic(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_logistic(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.72174 0.6673293914121201 0.3766778890297757 0.48154496841869915\n",
      "Validation\n",
      "0.72112 0.6601112026359144 0.37592353700011727 0.4790405738623627\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Logistic Regression SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best lambda for PRI_JET_NUM = 0 is 1e-05.\n",
      "The best lambda for PRI_JET_NUM = 1 is 1e-05.\n",
      "The best lambda for PRI_JET_NUM = 2 is 0.001.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    w_list = []\n",
    "    lambda_list = np.logspace(-10, 1, 12)\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    for lambda_ in lambda_list:\n",
    "        initial_w = np.random.rand(tx_tr_fe.shape[1], 1)\n",
    "        w, train_loss, dev_loss = reg_logistic_regression_plot(\n",
    "            y_tr, tx_tr_fe,\n",
    "            y_dev, tx_dev_fe,\n",
    "            lambda_,\n",
    "            initial_w,\n",
    "            max_iter,\n",
    "            learning_rate,\n",
    "            batch_size=1,\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        dev_losses.append(dev_loss)\n",
    "        w_list.append(w)\n",
    "\n",
    "    index = np.argmin(dev_losses)\n",
    "    best_lambda = lambda_list[index]\n",
    "    best_w = w_list[index]\n",
    "    print(\"The best lambda for PRI_JET_NUM = {} is {}.\".format(i, best_lambda))\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_logistic(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_logistic(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.71431 0.6464208242950108 0.36917202279451417 0.46995306035362433\n",
      "Validation\n",
      "0.7166 0.6460992907801418 0.37392986982526094 0.4737037587282722\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "578317ea115a054ac4bccdfa892faa859649678dbaf6df54b2b98fb2846a81cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
