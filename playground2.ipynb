{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "from utils.helpers import *\n",
    "from utils.prediction import *\n",
    "from utils.preprocess import *\n",
    "from utils.cross_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './data/train.csv'\n",
    "TEST_PATH = './data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 1e-3\n",
    "degree = 7\n",
    "learning_rate = 0.1\n",
    "max_iter = 1000\n",
    "optimizer = 'gd'\n",
    "k_fold = 5\n",
    "seed = 20221031\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw_tr, tx_raw_tr, ids_tr = load_csv_data(TRAIN_PATH)\n",
    "_, tx_raw_te, ids_te = load_csv_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 1)\n",
      "(250000, 29)\n",
      "(568238, 29)\n"
     ]
    }
   ],
   "source": [
    "y_tr = process_y(y_raw_tr)\n",
    "tx_tr = process_tx(tx_raw_tr)\n",
    "tx_te = process_tx(tx_raw_te)\n",
    "print(y_tr.shape)\n",
    "print(tx_tr.shape)\n",
    "print(tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 29)\n",
      "(50000, 29)\n",
      "(200000, 1)\n",
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "tx_tr, tx_dev, y_tr, y_dev = cross_validation_dataset(y_tr, tx_tr, k_indices, k=k_fold-1)\n",
    "print(tx_tr.shape)\n",
    "print(tx_dev.shape)\n",
    "print(y_tr.shape)\n",
    "print(y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split datasets to different jet nums\n",
    "# and remove columns with missing values for each jet num\n",
    "tx_train_list, y_tr_list = split_jet_num(tx_tr, y_tr)\n",
    "tx_dev_list, y_dev_list = split_jet_num(tx_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add polynomial features\n",
    "for i in range(3):\n",
    "    tx_train_list[i] = build_poly(tx_train_list[i], degree)\n",
    "    tx_dev_list[i] = build_poly(tx_dev_list[i], degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79917, 119) (19996, 119)\n",
      "(62257, 147) (15287, 147)\n",
      "(57826, 196) (14717, 196)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tx_train_list[i].shape, tx_dev_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [0, 0, 0]\n",
    "stds = [0, 0, 0]\n",
    "for i in range(3):\n",
    "    tx_train_list[i], tx_dev_list[i], means[i], stds[i] = normalization(\n",
    "        tx_train_list[i],\n",
    "        tx_dev_list[i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'means' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2293/3009889217.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_train_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_dev_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'means' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tx_train_list[i].shape, tx_dev_list[i].shape, means[i].shape, stds[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression_plot(y_tr, tx_tr, y_dev, tx_dev, lambda_, initial_w, max_iters, gamma, batch_size=8, optimizer='gd'):\n",
    "    \"\"\"Regularized logistic regression using gradient descent\n",
    "    or SGD (y ∈ {0, 1}, with regularization term λ|w|2)\n",
    "\n",
    "    Args:\n",
    "        y_tr: numpy array of shape=(N_tr, 1)\n",
    "        tx_tr: numpy array of shape=(N_tr, D)\n",
    "        y_dev: numpy array of shape=(N_dev, 1)\n",
    "        tx_dev: numpy array of shape=(N_dev, D)\n",
    "        lambda_: a scalar denoting the regularization term\n",
    "        initial_w: numpy array of shape=(D, 1). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        batch_size: mini batch size. default 8.\n",
    "        optimizer: 'gd' (batch sgd), 'ada' (adagrad), and 'adam'. default 'gd'.\n",
    "\n",
    "    Returns:\n",
    "        w: the best weight vector of shape (D, 1) for validation\n",
    "        train_loss: the corresponding mse loss\n",
    "        dev_loss: the corresponding mse loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    ws = [initial_w]\n",
    "    train_losses = [compute_ce(y_tr, tx_tr, w)]\n",
    "    dev_losses = [compute_ce(y_dev, tx_dev, w)]\n",
    "    grad_square_sum = 0\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps = 1e-9\n",
    "    m = 0\n",
    "    v = 0\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(\n",
    "            y_tr, tx_tr, batch_size=batch_size, num_batches=1\n",
    "        ):\n",
    "            # compute gradient\n",
    "            grad = logistic_reg_gradient(y_batch, tx_batch, w)\n",
    "\n",
    "            if optimizer == 'gd':\n",
    "                # update w by gradient\n",
    "                w = w - gamma * (grad + 2 * lambda_ * w)\n",
    "            elif optimizer == 'ada':\n",
    "                grad_square_sum += grad**2\n",
    "                adagrad = grad / np.sqrt(n_iter+1)\n",
    "                w = w - gamma * (adagrad + 2 * lambda_ * w)\n",
    "            elif optimizer == 'adam':\n",
    "                m = beta1*m + (1-beta1)*grad\n",
    "                v = beta2*v + (1-beta2)*grad**2\n",
    "                m_db = m / (1-beta1**(n_iter+1))\n",
    "                v_db = v / (1-beta2**(n_iter+1))\n",
    "                adam_grad = m_db / (np.sqrt(v_db) + eps)\n",
    "                w = w - gamma * (adam_grad + 2 * lambda_ * w)\n",
    "\n",
    "            # compute loss\n",
    "            loss = compute_ce(y_tr, tx_tr, w)\n",
    "\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "            # compute dev loss\n",
    "            dev_losses.append(compute_ce(y_dev, tx_dev, w))\n",
    "    \n",
    "    index = np.argmin(dev_losses)\n",
    "    plt.plot(np.arange(max_iters+1), train_losses, label='training_loss')\n",
    "    plt.plot(np.arange(max_iters+1), dev_losses, label='dev_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('iters')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('loss')\n",
    "    plt.savefig(str(tx_tr.shape[1]))\n",
    "    plt.clf()\n",
    "    return ws[index], train_losses[index], dev_losses[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best lambda for PRI_JET_NUM = 0 is 1.0.\n",
      "The best lambda for PRI_JET_NUM = 1 is 1.0.\n",
      "The best lambda for PRI_JET_NUM = 2 is 1.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    w_list = []\n",
    "    lambda_list = [i for i in np.logspace(-6, 2, 9)]\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    for lambda_ in lambda_list:\n",
    "        initial_w = np.random.rand(tx_tr_fe.shape[1], 1)\n",
    "        w, train_loss, dev_loss = reg_logistic_regression_plot(\n",
    "            y_tr, tx_tr_fe,\n",
    "            y_dev, tx_dev_fe,\n",
    "            lambda_,\n",
    "            initial_w,\n",
    "            max_iter*3,\n",
    "            0.001,\n",
    "            batch_size=tx_tr_fe.shape[0],\n",
    "            optimizer=optimizer\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        dev_losses.append(dev_loss)\n",
    "        w_list.append(w)\n",
    "\n",
    "    cross_validation_visualization(lambda_list, train_losses, dev_losses, i)\n",
    "    index = np.argmin(dev_losses)\n",
    "    best_lambda = lambda_list[index]\n",
    "    best_w = w_list[index]\n",
    "    print(\"The best lambda for PRI_JET_NUM = {} is {}.\".format(i, best_lambda))\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_logistic(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_logistic(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.74639 0.7040720885137447 0.44981271770655706 0.5489292828685259\n",
      "Validation\n",
      "0.74558 0.6979804441195284 0.4478714671044916 0.5456298889166696\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_plot(y_tr, tx_tr, y_dev, tx_dev, lambda_):\n",
    "    \"\"\"Ridge regression using normal equations.\n",
    "    Args:\n",
    "        y: numpy array of shape (N, 1), N is the number of samples.\n",
    "        tx: numpy array of shape (N, D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "\n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D, 1), D is the number of features.\n",
    "        loss: scalar\n",
    "    \"\"\"\n",
    "    N, D = tx_tr.shape\n",
    "    I = np.eye(D)\n",
    "    w = np.linalg.solve(tx_tr.T @ tx_tr + 2 * N * lambda_ * I, tx_tr.T @ y_tr).reshape(-1, 1)\n",
    "    train_loss = compute_mse(y_tr, tx_tr, w)\n",
    "    dev_loss = compute_mse(y_dev, tx_dev, w)\n",
    "\n",
    "    return w, train_loss, dev_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best lambda for PRI_JET_NUM = 0 is 0.\n",
      "The best lambda for PRI_JET_NUM = 1 is 0.\n",
      "The best lambda for PRI_JET_NUM = 2 is 0.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "dev_losses = []\n",
    "ws = []\n",
    "y_tr_pred, y_tr_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "y_dev_pred, y_dev_true = np.empty((0, 1)), np.empty((0, 1))\n",
    "\n",
    "for i in range(len(tx_train_list)):\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    w_list = []\n",
    "    lambda_list = [0]\n",
    "    # lambda_list = [1e-1]\n",
    "\n",
    "    y_tr = y_tr_list[i]\n",
    "    tx_tr_fe = tx_train_list[i]\n",
    "    y_dev = y_dev_list[i]\n",
    "    tx_dev_fe = tx_dev_list[i]\n",
    "\n",
    "    for lambda_ in lambda_list:\n",
    "        w, train_loss, dev_loss = ridge_regression_plot(\n",
    "            y_tr, tx_tr_fe,\n",
    "            y_dev, tx_dev_fe,\n",
    "            lambda_,\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        dev_losses.append(dev_loss)\n",
    "        w_list.append(w)\n",
    "\n",
    "    # cross_validation_visualization(lambda_list, train_losses, dev_losses, i)\n",
    "    index = np.argmin(dev_losses)\n",
    "    best_lambda = lambda_list[index]\n",
    "    best_w = w_list[index]\n",
    "    print(\"The best lambda for PRI_JET_NUM = {} is {}.\".format(i, best_lambda))\n",
    "\n",
    "    y_tr_pred = np.vstack((y_tr_pred, predict_linear(tx_tr_fe, best_w)))\n",
    "    y_dev_pred = np.vstack((y_dev_pred, predict_linear(tx_dev_fe, best_w)))\n",
    "    y_tr_true = np.vstack((y_tr_true, y_tr))\n",
    "    y_dev_true = np.vstack((y_dev_true, y_dev))\n",
    "    ws.append(best_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "0.80358 0.7449431258247173 0.6500080159736493 0.6942451082641927\n",
      "Validation\n",
      "0.80356 0.7417435486027544 0.6505805089715023 0.6931775584155943\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = compute_metrics(y_tr_true, y_tr_pred)\n",
    "print(\"Training\")\n",
    "print(accuracy, precision, recall, f1_score)\n",
    "\n",
    "accuracy, precision, recall, f1_score = compute_metrics(y_dev_true, y_dev_pred)\n",
    "print(\"Validation\")\n",
    "print(accuracy, precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = tx_te.shape[1]\n",
    "y_pred = np.empty((0, 1))\n",
    "for i in range(len(tx_te)):\n",
    "    pri_jet_num = np.min((2, int(tx_te[i, -1])))\n",
    "    w = ws[pri_jet_num]\n",
    "\n",
    "    if pri_jet_num == 0:\n",
    "        col_mask = np.ones(D, dtype=bool)\n",
    "        col_mask[[3, 4, 5, 11, 21, 22, 23, 24, 25, 26, 27, 28]] = False\n",
    "        tx_cleaned = tx_te[i, col_mask].reshape(1, -1)\n",
    "    elif pri_jet_num == 1:\n",
    "        col_mask = np.ones(D, dtype=bool)\n",
    "        col_mask[[3, 4, 5, 11, 25, 26, 27, 28]] = False\n",
    "        tx_cleaned = tx_te[i, col_mask].reshape(1, -1)\n",
    "    else:\n",
    "        tx_cleaned = tx_te[i, :-1].reshape(1, -1)\n",
    "\n",
    "    tx_cleaned = build_poly(tx_cleaned, degree)\n",
    "    # tx = ((tx_cleaned-means[pri_jet_num])/(stds[pri_jet_num]+1e-9)).reshape(1, -1)\n",
    "    tx = ((tx_cleaned-stds[pri_jet_num])/(means[pri_jet_num]-stds[pri_jet_num])).reshape(1, -1)\n",
    "    y_pred = np.vstack((y_pred, predict_linear(tx, w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred==0] = -1\n",
    "y_pred = y_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_te, y_pred, 'submission_linear_regression_cross_validation.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "578317ea115a054ac4bccdfa892faa859649678dbaf6df54b2b98fb2846a81cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
